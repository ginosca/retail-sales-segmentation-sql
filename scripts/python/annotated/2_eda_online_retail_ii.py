# ğŸ“Š Exploratory Data Analysis Script â€“ Online Retail II
# ğŸ““ Source Notebook: 2_eda_online_retail_ii.ipynb
# ğŸ” Description: Explores trends in sales, products, customers, and revenue.
# ğŸ« School: Ironhack Puerto Rico
# ğŸ“ Bootcamp: Data Science and Machine Learning
# ğŸ“… Date: December 20, 2024
# ğŸ‘©â€ğŸ’» Author: Ginosca Alejandro DÃ¡vila

#!/usr/bin/env python
# coding: utf-8

# # ğŸ“Š **Exploratory Data Analysis (EDA) for Online Retail II**
# ## **Data Science and Machine Learning Bootcamp â€“ Ironhack Puerto Rico**
# ### ğŸ““ Notebook: `2_eda_online_retail_ii.ipynb`
# ğŸ“… **Date:** December 20, 2024  
# ğŸ‘©â€ğŸ’» **Author:** Ginosca Alejandro DÃ¡vila  
# 
# ---
# 
# ## **ğŸ“Œ Notebook Overview**
# 
# This notebook performs an in-depth **Exploratory Data Analysis (EDA)** using the cleaned transactional data from the **Online Retail II** dataset.
# 
# ğŸ” _This EDA serves as a foundational step for segmentation and sales performance analysis, including RFM modeling and customer clustering._  
# ğŸ““ For full project scope and data cleaning steps, refer to the previous notebook: `1_data_cleaning_online_retail_ii.ipynb`.
# 
# ---
# 
# We aim to uncover key patterns in:
# - ğŸ›’ **Sales performance** over time  
# - ğŸŒ **Country-level revenue**  
# - ğŸ‘¤ **Customer behavior** and segmentation  
# - ğŸ“¦ **Top-performing products**  
# 
# This EDA directly supports the SQL-based analysis in `3_sql_analysis_sales_performance.ipynb`, where we re-answer these same business questions using relational queries.
# 
# ---
# 
# ## **ğŸ§  Business Questions Answered in Both EDA and SQL**
# 
# To enable direct comparison, this notebook is organized to align with the following 12 core business questions also answered in SQL:
# 
# ### ğŸ“ˆ Sales Performance
# 1. What is the monthly revenue trend from 2009 to 2011?  
# 2. What are the top 10 best-selling products by total revenue?  
# 3. Which invoices had the highest total transaction value?  
# 
# ### ğŸŒ Country & Regional Insights  
# 4. Which countries generate the most revenue?  
# 5. Do customer behaviors differ by country? *(e.g., avg. spend or frequency)*
# 
# ### ğŸ‘¤ Customer Insights  
# 6. How many customers made only one purchase?  
# 7. What is the average order value per customer?  
# 8. Who are the top 10 customers by total spend?
# 
# ### ğŸ§  Customer Segmentation (RFM Analysis)  
# 9. How recently has each customer made a purchase? *(Recency)*  
# 10. How frequently has each customer purchased? *(Frequency)*  
# 11. How much revenue has each customer generated? *(Monetary)*  
# 12. How can we segment customers based on RFM scores?
# 
# ---
# 
# ## **ğŸ“‚ Input Data**
# 
# ğŸ“ `cleaned_data/`  
# This folder contains cleaned datasets from `1_data_cleaning_online_retail_ii.ipynb`, including:
# 
# - âœ… `cleaned_online_retail_II.csv` â†’ Flat file used in this notebook  
# - âœ… `customers.csv` â†’ 1 row per customer  
# - âœ… `products.csv` â†’ Unique product catalog  
# - âœ… `invoices.csv` â†’ One row per invoice  
# - âœ… `invoice_items.csv` â†’ One row per product line in each invoice  
# 
# > ğŸ“Œ **Note**: This notebook uses only `cleaned_online_retail_II.csv`. The relational tables are used exclusively in the SQL analysis notebook.
# 
# ---
# 
# ## **ğŸ’¾ EDA Outputs Saved**
# 
# ğŸ“ `eda_outputs/`  
# â”œâ”€â”€ ğŸ“‚ `plots/` â†’ All `.png` chart files  
# â”œâ”€â”€ ğŸ“‚ `data/` â†’ Summary and aggregation `.csv` files  
# 
# These saved charts and aggregated datasets are used again for:
# - Direct comparison in `3_sql_analysis_sales_performance.ipynb` (EDA vs SQL results)  
# - Optional reuse in future segmentation work (e.g., RFM SQL implementation)
# 
# ---
# 
# ## **ğŸ¯ Goals**
# 
# âœ” Identify **sales patterns** and **seasonal trends**  
# âœ” Understand **regional differences in behavior**  
# âœ” Pinpoint **high-value customers and products**  
# âœ” Support **SQL query development** and **segmentation logic**  
# 
# ---
# 
# ğŸ” **Letâ€™s begin exploring the Online Retail II dataset!**
# 

# ---
# 
# ## ğŸ“‚ Step 1: Mount Google Drive
# 
# To begin, we need to mount Google Drive to access the project folder and load the cleaned flat dataset:
# 
# ğŸ“ `My Drive > Colab Notebooks > Ironhack > Week 3 > Week 3 - Day 4 > project-2-eda-sql > retail-sales-segmentation-sql > cleaned_data`
# 
# This notebook uses the following file for all EDA steps:
# 
# - âœ… `cleaned_online_retail_II.csv` â†’ Full cleaned dataset in flat structure (used throughout this notebook)
# 
# > ğŸ” **Note**: While normalized tables (`customers.csv`, `products.csv`, `invoices.csv`, `invoice_items.csv`) were also exported during data cleaning, they are **not used** here. Those are exclusively used in the SQL notebook (`3_sql_analysis_sales_performance.ipynb`).
# 
# ---

# In[1]:


import sys
import os

# âœ… Safe print to avoid encoding issues in some terminals
def safe_print(text):
    try:
        print(text)
    except UnicodeEncodeError:
        print(text.encode("ascii", errors="ignore").decode())

# âœ… Check if running inside Google Colab
def is_colab():
    return 'google.colab' in sys.modules

# ğŸ”§ Set base path depending on environment
if is_colab():
    from google.colab import drive
    drive.mount('/content/drive')

    # âœ… Default project path in Colab
    default_path = 'MyDrive/Colab Notebooks/Ironhack/Week 3/Week 3 - Day 4/project-2-eda-sql/retail-sales-segmentation-sql'
    full_default_path = os.path.join('/content/drive', default_path)

    if os.path.exists(full_default_path):
        project_base_path = full_default_path
        safe_print(f"âœ… Colab project path set to: {project_base_path}")
    else:
        safe_print("\nğŸ“‚ Default path not found. Please input the relative path to your project inside Google Drive.")
        safe_print("ğŸ‘‰ Example: 'MyDrive/Colab Notebooks/Ironhack/Week 3/Week 3 - Day 4/project-2-eda-sql/retail-sales-segmentation-sql'")
        user_path = input("ğŸ“¥ Your path: ").strip()
        project_base_path = os.path.join('/content/drive', user_path)

        if not os.path.exists(project_base_path):
            raise FileNotFoundError(f"âŒ Path does not exist: {project_base_path}\nPlease check your input.")

        safe_print(f"âœ… Colab project path set to: {project_base_path}")

else:
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
    except NameError:
        script_dir = os.getcwd()

    # Automatically walk up until project root (contains 'data/' and 'notebooks/')
    levels_up = 0
    while levels_up < 5:
        potential_root = os.path.abspath(os.path.join(script_dir, *(['..'] * levels_up)))
        if os.path.isdir(os.path.join(potential_root, 'data')) and os.path.isdir(os.path.join(potential_root, 'notebooks')):
            project_base_path = potential_root
            break
        levels_up += 1
    else:
        raise FileNotFoundError("âŒ Project root folder not found. Ensure it contains 'data' and 'notebooks' folders.")

    safe_print(f"âœ… Local environment detected. Base path set to: {project_base_path}")


# ---
# 
# ## ğŸ§° Step 2: Import Libraries and Load Cleaned Dataset
# 
# Next, weâ€™ll import the libraries required for data manipulation and visualization.  
# Then, weâ€™ll load the cleaned flat dataset that was exported in the previous notebook:
# 
# - âœ… `cleaned_online_retail_II.csv` â†’ Full cleaned transactional data with no missing values or canceled orders.
# 
# This file provides a reliable foundation for EDA, aggregation, and customer segmentation.  
# 
# > ğŸ§  **Note**: The normalized relational tables (`customers.csv`, `products.csv`, `invoices.csv`, `invoice_items.csv`) are used exclusively in the SQL notebook:  
# > ğŸ““ `3_sql_analysis_sales_performance.ipynb`, which answers the same business questions using relational queries.
# 
# ---
# 

# In[2]:


# ğŸ“¦ Import core libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import os

# ğŸ§¼ Aesthetic setup for plots
plt.style.use("ggplot")
sns.set_palette("pastel")

# ğŸ“ Define path to cleaned flat dataset
clean_path = os.path.join(project_base_path, 'cleaned_data')
full_data_path = os.path.join(clean_path, 'cleaned_online_retail_II.csv')

# ğŸ“¥ Load the flat dataset with error handling
try:
    cleaned_full_df = pd.read_csv(full_data_path, parse_dates=['invoice_date'])

    safe_print("âœ… Cleaned flat dataset loaded successfully.")
    safe_print(f"cleaned_full_df shape: {cleaned_full_df.shape}")

except FileNotFoundError as e:
    safe_print(f"âŒ File not found: {e}")
    sys.exit(1)


# ---
# 
# ### âœ… Cleaned Dataset Loaded Successfully
# 
# The cleaned flat file `cleaned_online_retail_II.csv` has been successfully loaded from the `cleaned_data/` directory.
# 
# This dataset contains:
# - One row per **transaction line** (i.e., each product sold in an invoice)
# - All relevant fields merged from the original source, including:
#   - `invoice_no`, `stock_code`, `description`, `quantity`
#   - `invoice_date`, `unit_price`, `customer_id`, `country`, `line_revenue`
# 
# This flat structure is ideal for **exploratory analysis**, **visualization**, and **aggregation** using Python.
# 
# ---
# 

# ---
# 
# ## ğŸ§¾ Step 3: Dataset Preview & Structure Overview
# 
# In this step, weâ€™ll inspect the structure of the main flat dataset:  
# ğŸ“„ `cleaned_online_retail_II.csv`
# 
# Weâ€™ll review:
# - The **first and last few rows** (sample data preview)  
# - The **shape** of the dataset (rows Ã— columns)  
# - **Column names**, **data types**, and **non-null counts**  
# - Any **missing values** and their distribution
# 
# This structure validation helps confirm that the dataset was exported properly and is ready for analysis.
# 
# 
# ---

# In[3]:


import io

# âœ… Display fallback for script environments
try:
    display
except NameError:
    def display(x):
        print(x.to_string() if isinstance(x, pd.DataFrame) else str(x))

# ğŸ” Helper function to inspect a DataFrameâ€™s basic structure
def inspect_basic_structure(df, name="Dataset", preview_rows=5):
    """
    Display structure, sample rows, missing values, and schema of a DataFrame.
    Compatible with both notebooks and terminal scripts.
    """
    safe_print(f"ğŸ§¾ Inspecting: {name}")
    safe_print("=" * 60)

    # ğŸ‘ï¸ Preview first N rows
    safe_print(f"ğŸ”¹ First {preview_rows} Rows:")
    display(df.head(preview_rows))

    # ğŸ‘ï¸ Preview last N rows
    safe_print(f"\nğŸ”¹ Last {preview_rows} Rows:")
    display(df.tail(preview_rows))

    # ğŸ“ Dataset shape
    safe_print(f"\nğŸ”¹ Shape: {df.shape[0]} rows Ã— {df.shape[1]} columns")

    # ğŸ·ï¸ Column names
    safe_print("\nğŸ”¹ Column Names:")
    safe_print(df.columns.tolist())

    # ğŸ§¬ Data types and non-null counts
    safe_print("\nğŸ”¹ Data Types and Non-Null Counts:")
    buffer = io.StringIO()
    df.info(buf=buffer)
    safe_print(buffer.getvalue())

    # ğŸ§¼ Missing values per column
    safe_print("ğŸ”¸ Missing Values (per column):")
    missing_summary = df.isnull().sum()
    display(missing_summary[missing_summary > 0] if missing_summary.any() else "âœ… No missing values.")

    safe_print("=" * 60 + "\n")


# In[4]:


# ğŸ§ª Inspect structure and content of the cleaned_full_df DataFrame
inspect_basic_structure(cleaned_full_df, name="cleaned_online_retail_II.csv")


# ---
# 
# ### ğŸ§¾ Summary: `cleaned_online_retail_II.csv`
# 
# The dataset has been successfully loaded and contains **766,226 rows** and **9 columns**, with each row representing an item-level transaction within an invoice.
# 
# Key observations:
# 
# - âœ… **No missing values** â€” all fields are clean and analysis-ready  
# - ğŸ“… `invoice_date` is correctly parsed as a `datetime64[ns]` object  
# - ğŸ’° `line_revenue` (calculated as `quantity Ã— unit_price`) is included and ready for revenue analysis  
# - ğŸ”¤ Categorical text fields like `description` and `country` are stored as `object` types  
# - ğŸ§¾ Identifiers such as `invoice_no`, `stock_code`, and `customer_id` are correctly typed as integers or strings
# 
# This flat, denormalized dataset will be the primary source for all **exploratory**, **time-based**, **customer-level**, and **product-level** insights in this notebook.
# 
# ---
# 

# ---
# 
# ## ğŸ§© Step 4: Variable Classification â€“ `cleaned_online_retail_II.csv`
# 
# In this step, we classify the columns from the cleaned full dataset into one of four roles:
# 
# - ğŸ†” **Identifiers** â€“ Unique keys or foreign keys used for grouping or tracking  
# - ğŸ”¤ **Categorical** â€“ Labels or text fields used for grouping, filtering, or segmentation  
# - ğŸ“Š **Quantitative** â€“ Numeric columns used for aggregations or distribution analysis  
# - ğŸ“… **Datetime** â€“ Timestamps used for time-based trend analysis
# 
# This classification helps define how each variable should be handled during EDA and downstream modeling.
# 
# ğŸ“„ **Dataset:** `cleaned_online_retail_II.csv`  
# ğŸ”¢ **Shape:** 766,226 rows Ã— 9 columns
# ğŸ“ **Note:** This dataset includes one row per item line in a transaction and will serve as the **primary source** for all EDA and business question analysis.
# 
# Weâ€™ll now define column roles explicitly for clarity.
# 
# 
# ---

# In[5]:


# ğŸ§  Variable classification for cleaned_full_df only
variable_types = {
    'cleaned_online_retail_II': {
        'identifier': ['invoice_no', 'stock_code', 'customer_id'],
        'categorical': ['description', 'country'],
        'quantitative': ['quantity', 'unit_price', 'line_revenue'],
        'datetime': ['invoice_date']
    }
}

# âœ… Display summary
safe_print("âœ… Variable classification completed:\n")

for dataset, types in variable_types.items():
    safe_print(f"ğŸ“„ Dataset: {dataset}")
    for var_type, columns in types.items():
        safe_print(f"  {var_type.title()} Columns:")
        if columns:
            for col in columns:
                safe_print(f"    â€¢ {col}")
        else:
            safe_print("    (None)")
    safe_print("")


# ---
# 
# ### ğŸ§¾ Summary: Variable Classification
# 
# Weâ€™ve classified all variables from the primary dataset `cleaned_online_retail_II.csv` into one of four roles:
# 
# - ğŸ†” Identifier columns  
# - ğŸ”¤ Categorical columns  
# - ğŸ“Š Quantitative columns  
# - ğŸ“… Datetime columns  
# 
# This classification guides appropriate transformations, aggregations, and visualizations for EDA.
# 
# ---
# 
# #### ğŸ“„ `cleaned_online_retail_II.csv`
# 
# | Variable        | Type         |
# |-----------------|--------------|
# | `invoice_no`    | Identifier   |
# | `stock_code`    | Identifier   |
# | `customer_id`   | Identifier   |
# | `description`   | Categorical  |
# | `country`       | Categorical  |
# | `quantity`      | Quantitative |
# | `unit_price`    | Quantitative |
# | `line_revenue`  | Quantitative |
# | `invoice_date`  | Datetime     |
# 
# ---
# 
# This structure will help us explore product trends, customer behavior, and time-based performance patterns more effectively in the upcoming steps.
# 
# ---
# 

# ---
# 
# ## ğŸ“Š Step 5: Descriptive Statistics for Quantitative Columns
# 
# We begin by exploring the central tendency and distribution of the main numeric variables in the dataset:
# 
# - `quantity`: Number of units purchased per item  
# - `unit_price`: Selling price per unit (in GBP)  
# - `line_revenue`: Revenue per item line (`quantity Ã— unit_price`)
# 
# These statistics help us assess the scale of transactions and detect potential outliers, which are common in wholesale or bulk order environments.
# 
# ---
# 

# In[6]:


# ğŸ”¢ Columns to describe
quantitative_cols = ['quantity', 'unit_price', 'line_revenue']

# ğŸ“Š Summary statistics
safe_print("ğŸ“Š Summary Statistics for Quantitative Columns:\n")
display(cleaned_full_df[quantitative_cols].describe())

# ğŸ” Check for zero or negative values (if any remain)
safe_print("\nğŸ” Number of rows with non-positive values:")
for col in quantitative_cols:
    count = (cleaned_full_df[col] <= 0).sum()
    safe_print(f"  â€¢ {col}: {count}")


# ---
# 
# ### ğŸ“Š Summary: Quantitative Variables
# 
# The table below summarizes the distribution of all numeric columns in `cleaned_online_retail_II.csv`.
# 
# | Metric       | `quantity` | `unit_price` | `line_revenue` |
# |--------------|------------|--------------|----------------|
# | **Count**    | 766,226    | 766,226      | 766,226        |
# | **Mean**     | 13.70      | Â£2.95        | Â£22.28         |
# | **Std Dev**  | 147.33     | Â£4.35        | Â£227.46        |
# | **Min**      | 1          | Â£0.001       | Â£0.001         |
# | **25%**      | 2          | Â£1.25        | Â£5.00          |
# | **Median**   | 6          | Â£1.95        | Â£12.50         |
# | **75%**      | 12         | Â£3.75        | Â£19.80         |
# | **Max**      | 80,995     | Â£649.50      | Â£168,469.60    |
# 
# Key insights:
# 
# - âš ï¸ **Extreme outliers** are present, especially in `quantity` and `line_revenue`. These likely correspond to large wholesale orders.
# - âœ… No zero or negative values â€” all numeric entries are valid for analysis.
# - ğŸ’° Most transactions involve **low quantities** and **low unit prices**, with a **median total price of ~Â£12.50**.
# 
# Weâ€™ll now visualize these distributions to assess skewness and decide if transformations (like `log1p`) are needed.
# 
# ---
# 

# ---
# 
# ## ğŸ“Š Step 6: Distribution of Quantitative Variables (with Frequency Tables)
# 
# To explore the behavior and skewness of our main numeric fields, we analyze:
# 
# - ğŸ“‹ **Frequency Tables** (before each chart):
#   - First, the raw values
#   - Then, log-transformed values using `log1p()` to reduce skewness and compress outliers
# 
# - ğŸ“ˆ **Histograms** (side-by-side):
#   - Left: Raw distribution
#   - Right: Log-transformed distribution
# 
# These tables and visualizations allow us to:
# - Understand which values dominate each variable
# - Spot long-tailed or right-skewed distributions
# - Justify the use of log transformation for outlier-heavy variables
# 
# Each histogram pair is saved in the project under:  
# ğŸ“ `eda_outputs/plots/`
# 
# > ğŸ“Œ **Insight Tip**: If most of the data falls in the first few bins and only a handful in the rest, the variable is highly skewed and may benefit from transformation or trimming for modeling.
# 
# ---
# 

# In[7]:


import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ğŸ“¦ Setup
OVERWRITE_PLOTS = True
plot_export_dir = os.path.join(project_base_path, 'eda_outputs', 'plots')
os.makedirs(plot_export_dir, exist_ok=True)

# ğŸ”¢ Quantitative columns to analyze
quant_cols = ['quantity', 'unit_price', 'line_revenue']
plot_index = 1

# ğŸ“‹ Frequency table helper
def display_frequency_table(series, label, bins=15, log_scale=False):
    title = f"log1p({label})" if log_scale else label
    safe_print(f"\nğŸ“‹ Frequency Table for: {title}")
    safe_print("=" * 50)

    # Transform if log
    data = np.log1p(series) if log_scale else series

    if pd.api.types.is_integer_dtype(data) and data.nunique() < 50:
        freq_table = data.value_counts().sort_index()
    else:
        freq_table = pd.cut(data, bins=bins).value_counts().sort_index()

    display(freq_table)

# ğŸ” Loop through columns
for col in quant_cols:
    series = cleaned_full_df[col]

    # ğŸ“‹ Raw frequency table
    display_frequency_table(series, col, bins=15, log_scale=False)

    # ğŸ“‹ Log1p frequency table
    display_frequency_table(series, col, bins=15, log_scale=True)

    # ğŸ¨ Plot setup
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    fig.suptitle(f'Distribution of {col}', fontsize=16, fontweight='bold')

    # Axis label with units
    if col in ['unit_price', 'line_revenue']:
        x_label = f"{col} (GBP Â£)"
    else:
        x_label = col

    # Raw histogram
    sns.histplot(series, bins=100, ax=axes[0], kde=False)
    axes[0].set_title(f'{col} (Raw)', fontsize=12)
    axes[0].set_xlabel(x_label)
    axes[0].set_ylabel('Frequency')

    # Log-transformed histogram
    sns.histplot(np.log1p(series), bins=100, ax=axes[1], kde=False)
    axes[1].set_title(f'{col} (Log Scale)', fontsize=12)
    axes[1].set_xlabel(f'log1p({x_label})')
    axes[1].set_ylabel('Frequency')

    # Save plot
    plot_filename = f"{plot_index:02d}_{col}_distribution.png"
    plot_path = os.path.join(plot_export_dir, plot_filename)
    if OVERWRITE_PLOTS or not os.path.exists(plot_path):
        plt.savefig(plot_path)
        safe_print(f"âœ… Saved plot: {plot_path}")
    else:
        safe_print(f"âš ï¸ Skipped (already exists): {plot_path}")

    plt.show()
    plot_index += 1


# ---
# 
# ### ğŸ“Š Summary: Distribution Analysis
# 
# We examined the distribution of the three main quantitative variables in `cleaned_online_retail_II.csv` using both raw and log-transformed views.
# 
# | Column         | Key Characteristics |
# |----------------|----------------------|
# | `quantity`     | Right-skewed distribution. Most transactions fall below **5,400 units**, with just **two extreme outliers** reaching the max of **80,995 units**. |
# | `unit_price`   | Most products are priced under **Â£43**, though there are rare outliers up to **Â£649.50**. |
# | `line_revenue` | Nearly all transaction lines are under **Â£11,231**, with a handful of extreme outliers â€” the largest reaching **Â£168,469.60**. |
# 
# Each variable was analyzed through:
# - ğŸ“‹ **Raw frequency tables**
# - ğŸ“‹ **Log-transformed frequency tables** using `log1p()`
# - ğŸ“ˆ **Histograms** (raw and log scale)
# 
# These analyses help visualize skewness, support outlier detection, and guide the decision to apply log transformations in downstream modeling.
# 
# ---
# 
# ### âœ… Outputs Saved
# 
# All plots were saved under:
# 
# ğŸ“ `eda_outputs/plots/`  
# File names:
# - `01_quantity_distribution.png`  
# - `02_unit_price_distribution.png`  
# - `03_line_revenue_distribution.png`
# 
# These visuals support the upcoming steps for analyzing trends, customer behavior, and product performance.
# 
# ---
# 

# ---
# 
# ## ğŸ§  Step 7: Begin Answering Business Questions (EDA Perspective)
# 
# Now that weâ€™ve completed the core Exploratory Data Analysis (EDA), weâ€™ll begin applying it to directly answer the **12 business questions** outlined in our SQL-based analysis notebook:  
# ğŸ““ `3_sql_analysis_sales_performance.ipynb`
# 
# In this section, weâ€™ll use Python and Pandas to answer the same questions for comparison and validation.
# 
# We will explore:
# 
# - ğŸ“ˆ **Sales performance** across time, customers, and products  
# - ğŸŒ **Country-level insights**  
# - ğŸ‘¤ **Customer purchasing behavior**  
# - ğŸ§® **Customer segmentation using RFM metrics**
# 
# > ğŸ§© Note: If a question has already been addressed during general EDA, we will reference the earlier section and avoid repeating identical plots or calculations.  
# >  
# > ğŸ“ Outputs from this section are saved in `eda_outputs/plots/` and `eda_outputs/data/` for consistency across notebooks and dashboards.
# 
# ---
# 
# ### ğŸš€ Letâ€™s begin by aligning with the first business question on monthly revenue trends!
# 

# ---
# 
# ## ğŸ“ˆ Q1: Monthly Revenue Trend (2009â€“2011)
# 
# To identify trends and seasonality in overall business performance, we calculate the **monthly revenue** by aggregating `line_revenue` for each calendar month.
# 
# ### ğŸ§® Methodology
# - Convert `invoice_date` to `YYYY-MM` format
# - Group by month
# - Aggregate:
#   - Total revenue: sum of `line_revenue`
#   - Invoice count per month
#   - Average revenue per invoice
# - Format month labels for readability (e.g., `2009-12`)
# - Mark **December 2011** with an asterisk (`*`) to indicate it's a **partial month**
# - Add vertical lines to separate each year
# 
# > ğŸ“‚ Output:
# > - Data export: `eda_outputs/data/01_monthly_revenue_trend.csv`
# > - Plot export: `eda_outputs/plots/04_monthly_revenue_trend.png`
# 
# This analysis helps highlight:
# - Seasonal peaks (e.g., **Q4 holiday surges**)
# - Long-term revenue trends across years
# - The **partial nature of the final month**, which must be accounted for when interpreting the final drop in revenue
# 
# ---
# 

# In[8]:


import pandas as pd
import matplotlib.pyplot as plt
import os
import matplotlib.dates as mdates

# âœ… Setup export behavior
OVERWRITE_PLOTS = True
OVERWRITE_CSV = True
plot_index = 4
data_index = 1

# ğŸ“ Define output directories
plot_dir = os.path.join(project_base_path, 'eda_outputs', 'plots')
data_dir = os.path.join(project_base_path, 'eda_outputs', 'data')
os.makedirs(plot_dir, exist_ok=True)
os.makedirs(data_dir, exist_ok=True)

# ğŸ“… Extract invoice_month from invoice_date
cleaned_full_df['invoice_month'] = cleaned_full_df['invoice_date'].dt.to_period('M').dt.to_timestamp()
monthly_summary = cleaned_full_df.groupby('invoice_month').agg(
    monthly_revenue=('line_revenue', 'sum'),
    monthly_invoices=('invoice_no', 'nunique')
).reset_index()
monthly_summary['avg_revenue_per_invoice'] = monthly_summary['monthly_revenue'] / monthly_summary['monthly_invoices']

# ğŸ“ Add label for partial month (December 2011)
monthly_summary['invoice_month_str'] = monthly_summary['invoice_month'].dt.strftime('%Y-%m')
monthly_summary.loc[monthly_summary['invoice_month_str'] == '2011-12', 'invoice_month_str'] += ' *'

# ğŸ’¾ Export summary table
monthly_data_path = os.path.join(data_dir, f'{data_index:02d}_monthly_revenue_summary.csv')
if OVERWRITE_CSV or not os.path.exists(monthly_data_path):
    monthly_summary.to_csv(monthly_data_path, index=False)
    safe_print(f"âœ… Exported summary: {monthly_data_path}")
else:
    safe_print(f"âš ï¸ Skipped (already exists): {monthly_data_path}")

# ğŸ“‹ Show the full table
display(monthly_summary[['invoice_month_str', 'monthly_revenue', 'monthly_invoices', 'avg_revenue_per_invoice']])

# ğŸ“ Note about partial month
safe_print("\nğŸ“ Note: Months marked with * are partial months.")
safe_print("- 2011-12: Data available only up to December 9th.\n")

# ğŸ“Š Plot monthly revenue trend
fig, ax = plt.subplots(figsize=(14, 6))
ax.plot(monthly_summary['invoice_month'], monthly_summary['monthly_revenue'], marker='o', linewidth=2, label='Monthly Revenue')

# ğŸ§­ Format x-axis ticks to show all months
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))
ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
plt.xticks(rotation=45)

# ğŸ“ Add vertical lines for each new year
for date in monthly_summary['invoice_month']:
    if date.month == 1:
        ax.axvline(date, color='gray', linestyle='--', alpha=0.7, label='Year Start' if date.year == 2010 else "")

# ğŸ“ Mark partial month (Dec 2011)
partial_month = pd.to_datetime('2011-12-01')
ax.axvline(partial_month, color='red', linestyle='--', linewidth=1.5, label='Partial Month')

# ğŸ·ï¸ Labels and legend
ax.set_title('Monthly Revenue Trend (2009â€“2011)', fontsize=16, fontweight='bold')
ax.set_xlabel('Invoice Month')
ax.set_ylabel('Total Revenue (GBP Â£)')
ax.legend()
plt.tight_layout()

# ğŸ’¾ Save plot
plot_path = os.path.join(plot_dir, f'{plot_index:02d}_monthly_revenue_trend.png')
if OVERWRITE_PLOTS or not os.path.exists(plot_path):
    plt.savefig(plot_path)
    safe_print(f"âœ… Saved plot: {plot_path}")
else:
    safe_print(f"âš ï¸ Skipped (already exists): {plot_path}")

plt.show()


# ---
# 
# ### â“ Q1. What is the monthly revenue trend from 2009 to 2011?
# 
# We calculated the **monthly revenue trend** using the `invoice_date` and `line_revenue` columns from the cleaned dataset.
# 
# #### ğŸ” Key Insights:
# - ğŸ“ˆ **Revenue peaked in November** of both **2010 (Â£1.16M)** and **2011 (Â£1.14M)**, reflecting strong **pre-holiday shopping activity**.
# - ğŸ“‰ A consistent **post-holiday dip** is visible in **January and February**, aligning with typical retail seasonality.
# - ğŸ” The pattern of **Q4 surges** followed by **Q1 slowdowns** shows clear annual seasonality in consumer behavior.
# - ğŸ“… The final month, **December 2011**, appears lower due to **partial data** â€” only sales up to **December 9th** are included. It should not be interpreted as an actual revenue drop.
# 
# > These results help identify peak periods and support strategic decisions in inventory management, marketing, and forecasting.
# 
# ---
# 

# ---
# 
# ## ğŸ“¦ Q2: Top 10 Best-Selling Products by Total Revenue
# 
# To identify the most profitable products, we calculate the **total revenue generated per product line**, combining product ID and description for clarity.
# 
# ### ğŸ§® Methodology
# - Group by product (`stock_code`, `description`)
# - Aggregate:
#   - Total revenue: sum of `line_revenue`
#   - Total quantity sold
#   - Average unit price
# - Sort in descending order by total revenue
# - Display the **top 10 products** ranked by revenue
# 
# > ğŸ“‚ Output:
# > - Data export: `eda_outputs/data/02_top_products_by_revenue.csv`
# > - Plot export: `eda_outputs/plots/05_top_products_revenue.png`
# 
# This analysis highlights the SKUs that contribute the most to overall revenue and supports decisions around **product promotion**, **inventory stocking**, and **sales prioritization**.
# 
# ---
# 

# In[9]:


import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import os

# âœ… Export flags
OVERWRITE_PLOTS = True
OVERWRITE_CSV = True

# ğŸ“ Define export paths
data_dir = os.path.join(project_base_path, 'eda_outputs', 'data')
plot_dir = os.path.join(project_base_path, 'eda_outputs', 'plots')
os.makedirs(data_dir, exist_ok=True)
os.makedirs(plot_dir, exist_ok=True)

# ğŸ“„ Output filenames
csv_path = os.path.join(data_dir, '02_top_products_by_revenue.csv')
plot_path = os.path.join(plot_dir, '05_top_products_revenue.png')

# ğŸ§® Aggregate top 10 products by revenue
top_products_df = (
    cleaned_full_df.groupby(['stock_code', 'description'])
    .agg(
        total_revenue=('line_revenue', 'sum'),
        total_quantity=('quantity', 'sum'),
        avg_unit_price=('unit_price', 'mean')
    )
    .reset_index()
    .sort_values(by='total_revenue', ascending=False)
    .head(10)
)

# ğŸ’¾ Save CSV
if OVERWRITE_CSV or not os.path.exists(csv_path):
    top_products_df.to_csv(csv_path, index=False)
    safe_print(f"âœ… Exported top products summary: {csv_path}")
else:
    safe_print(f"âš ï¸ Skipped (already exists): {csv_path}")

# ğŸ“‹ Show top products table
display(top_products_df)

# ğŸ¨ Plot setup
plt.figure(figsize=(12, 6))
pastel_colors = sns.color_palette("pastel", len(top_products_df))

# ğŸ“Š Draw horizontal bars manually
for i, (desc, revenue) in enumerate(zip(top_products_df["description"], top_products_df["total_revenue"])):
    plt.barh(y=i, width=revenue, color=pastel_colors[i])
    plt.text(revenue + 2000, i, f"Â£{revenue:,.0f}", va='center', fontsize=9)

# ğŸ·ï¸ Format plot
plt.yticks(ticks=range(len(top_products_df)), labels=top_products_df["description"])
plt.title("Top 10 Best-Selling Products by Revenue", fontsize=14, fontweight='bold')
plt.xlabel("Total Revenue (GBP Â£)")
plt.ylabel("Product Description")
plt.tight_layout()

# ğŸ’¾ Save plot
if OVERWRITE_PLOTS or not os.path.exists(plot_path):
    plt.savefig(plot_path, bbox_inches='tight')
    safe_print(f"âœ… Saved plot: {plot_path}")
else:
    safe_print(f"âš ï¸ Skipped (already exists): {plot_path}")

# ğŸ“ˆ Show
plt.show()


# ---
# 
# ### â“ Q2: Top 10 Best-Selling Products by Revenue
# 
# The chart and table above highlight the **top 10 products by total revenue** between 2009 and 2011.
# 
# #### ğŸ” Key Insights:
# 
# - ğŸ¥‡ The top product, **regency cakestand 3 tier**, generated **Â£277,656.25** in revenue from **24,124 units** sold, with an average unit price of **Â£12.46**.
# - ğŸ•¯ï¸ In second place, **white hanging heart t-light holder** earned **Â£247,048.01** from **91,757 units** â€” a **low-cost, high-volume** product that sold exceptionally well.
# - ğŸ¨ Many other high-revenue products were **affordable, decorative items**, such as:
#   - *paper craft, little birdie*
#   - *assorted colour bird ornament*
#   - *jumbo bag red retrospot*
# 
# These findings reinforce a **giftware and dÃ©cor-focused product strategy**, where small, visually appealing, budget-friendly items dominate sales performance.
# 
# ---
# 

# ---
# 
# ## ğŸ§¾ Q3: Invoices with the Highest Transaction Value
# 
# To identify the most valuable transactions, we calculate the **total revenue per invoice** by aggregating `line_revenue` across all items within each `invoice_no`.
# 
# ### ğŸ§® Methodology
# - Group by invoice (`invoice_no`)
# - Aggregate:
#   - Total revenue: sum of `line_revenue`
#   - Number of product lines: count of rows per invoice
#   - Include `customer_id` and `invoice_date` for context
# - Sort in descending order by total revenue
# - Display the **top 10 invoices**
# 
# > ğŸ“‚ Output:
# > - Data export: `eda_outputs/data/03_top_invoices_by_value.csv`
# > - Plot export: `eda_outputs/plots/06_top_invoices_value.png`
# 
# This analysis highlights **high-value transactions**, often associated with **bulk buyers** or **business clients**. These invoices can be strategically valuable for loyalty initiatives, special offers, or client segmentation.
# 
# ---
# 

# In[10]:


# ğŸ“¦ Libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# ğŸ“ Setup
OVERWRITE_PLOTS = True
plot_index = 6
data_index = 3
plot_dir = os.path.join(project_base_path, 'eda_outputs', 'plots')
data_dir = os.path.join(project_base_path, 'eda_outputs', 'data')
os.makedirs(plot_dir, exist_ok=True)
os.makedirs(data_dir, exist_ok=True)

# ğŸ§® Q3: Top 10 Invoices by Total Value
invoice_summary_df = (
    cleaned_full_df.groupby('invoice_no', as_index=False)
    .agg(
        total_invoice_revenue=('line_revenue', 'sum'),
        invoice_items=('stock_code', 'count'),
        customer_id=('customer_id', 'first'),
        invoice_date=('invoice_date', 'first')
    )
    .sort_values(by='total_invoice_revenue', ascending=False)
    .head(10)
)

# ğŸ’¾ Export
invoice_summary_path = os.path.join(data_dir, f"{data_index:02d}_top_invoices_by_value.csv")
invoice_summary_df.to_csv(invoice_summary_path, index=False)
print(f"âœ… Exported top invoice summary: {invoice_summary_path}")

# ğŸ“‹ Display summary table
from IPython.display import display
display(invoice_summary_df)

# ğŸ“Š Plot
plt.figure(figsize=(12, 6))
ax = sns.barplot(
    data=invoice_summary_df,
    x='invoice_no',
    y='total_invoice_revenue',
    hue='invoice_no',       # Required to silence deprecation warning
    order=invoice_summary_df.sort_values("total_invoice_revenue", ascending=False)["invoice_no"],
    palette='pastel',
    legend=False           # Hide legend since x already encodes this
)

plt.title('Top 10 Invoices by Total Value', fontsize=14, fontweight='bold')
plt.xlabel('Invoice Number')
plt.ylabel('Total Revenue (GBP Â£)')

# â• Add labels
for p in ax.patches:
    value = p.get_height()
    ax.annotate(
        f"Â£{value:,.0f}",
        (p.get_x() + p.get_width() / 2, value),
        ha='center', va='bottom', fontsize=9, fontweight='bold'
    )

# ğŸ’¾ Save plot
invoice_plot_path = os.path.join(plot_dir, f"{plot_index:02d}_top_invoices_by_value.png")
if OVERWRITE_PLOTS or not os.path.exists(invoice_plot_path):
    plt.savefig(invoice_plot_path, bbox_inches='tight')
    print(f"âœ… Saved plot: {invoice_plot_path}")
else:
    print(f"âš ï¸ Skipped (already exists): {invoice_plot_path}")

plt.show()


# ---
# 
# ### â“ Q3: Top 10 Invoices by Total Value
# 
# This analysis identifies the **10 highest-value invoices** in the dataset, based on total revenue from all products in each invoice.
# 
# | Metric                   | Insight                                                                                     |
# |--------------------------|---------------------------------------------------------------------------------------------|
# | ğŸ§¾ **Highest Invoice**    | Invoice `581483` generated **Â£168,469.60** from a **single item** â€” an extreme outlier, likely a bulk order or possible data anomaly. |
# | ğŸ’° **Invoice Range**      | Top 10 invoices range from **Â£22,206.00** to **Â£168,469.60** in total revenue. Most fall between **Â£22k and Â£77k**. |
# | ğŸ“¦ **Invoice Size**       | Item counts vary widely â€” some invoices contain over **130 items**, while others list only **1â€“2 items**. |
# | ğŸ‘¤ **Repeat Customers**   | Customers `18102` and `17450` appear multiple times, suggesting **loyal, high-value clients** with repeated large purchases. |
# | ğŸ—“ï¸ **Invoice Timing**     | These high-value invoices are distributed throughout **2010 and 2011**, without a clear seasonal pattern. |
# | âš ï¸ **Potential Anomalies**| The top invoice and other unusually high-value entries warrant **manual review** for data accuracy. |
# 
# This analysis highlights **large individual transactions**, which may reflect:
# - High-value **B2B or wholesale orders**
# - One-time **event-based purchases**
# - Potential **data entry errors or unusual pricing strategies**
# 
# ---
# 

# ---
# 
# ## ğŸŒ Q4: Revenue by Country (With and Without United Kingdom)
# 
# To understand the **geographic distribution of revenue** from 2009 to 2011, we analyze total sales grouped by `country`, both **including and excluding the United Kingdom**, which represents the primary market in this dataset.
# 
# ### ğŸ§® Methodology
# - Group transactions by `country`
# - Aggregate:
#   - `total_revenue`: sum of `line_revenue`
#   - `num_invoices`: count of unique `invoice_no`
#   - `avg_invoice_value`: total revenue divided by number of invoices
# - Sort countries in descending order of revenue
# - Generate two views:
#   - ğŸŒ Full view including UK
#   - ğŸŒ International-only view excluding UK
# 
# > ğŸ“‚ Output:
# > - Data export:
# >   - `04_revenue_by_country.csv` (all countries)
# >   - `04_revenue_by_country_excl_uk.csv` (excluding UK)
# > - Plot export:
# >   - `07_country_revenue_bar.png`
# >   - `07_country_revenue_bar_excl_uk.png`
# 
# This analysis highlights the **top-performing countries overall**, and isolates **international revenue potential** by separating out the dominant UK market.
# 
# ---
# 

# In[11]:


import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns

# ğŸ“ Export directories
data_export_path = os.path.join(project_base_path, 'eda_outputs', 'data')
plot_export_path = os.path.join(project_base_path, 'eda_outputs', 'plots')
os.makedirs(data_export_path, exist_ok=True)
os.makedirs(plot_export_path, exist_ok=True)

# ğŸ“Š Full revenue summary by country
country_summary_df = (
    cleaned_full_df.groupby('country')
    .agg(
        total_revenue=('line_revenue', 'sum'),
        num_invoices=('invoice_no', 'nunique')
    )
    .assign(avg_invoice_value=lambda df: df.total_revenue / df.num_invoices)
    .sort_values('total_revenue', ascending=False)
    .reset_index()
)

# ğŸ’¾ Export full country summary
full_country_path = os.path.join(data_export_path, '04_revenue_by_country.csv')
country_summary_df.to_csv(full_country_path, index=False)
safe_print(f"âœ… Exported revenue by country: {full_country_path}")

# ğŸ“‹ Display top 10 including UK
safe_print("\nğŸ“‹ Top 10 Countries by Revenue (Including UK):")
display(country_summary_df.head(10))

# ğŸ“Š Plot including UK
plt.figure(figsize=(12, 6))
ax = sns.barplot(
    data=country_summary_df.head(10),
    y='country',
    x='total_revenue',
    hue='country',  # âœ… Avoid warning
    legend=False,
    palette='pastel'
)
plt.title("Top 10 Countries by Revenue (Including UK)", fontsize=14)
plt.xlabel("Total Revenue (GBP Â£)")
plt.ylabel("Country")

# ğŸ’¬ Add labels at end of bars
for container in ax.containers:
    ax.bar_label(container, fmt='Â£%.0f', label_type='edge', padding=5)

plt.tight_layout()
plot_path_all = os.path.join(plot_export_path, '07_country_revenue_bar.png')
plt.savefig(plot_path_all)
safe_print(f"âœ… Saved plot: {plot_path_all}")
plt.show()

# --- ğŸŒ Version excluding UK ---
country_excl_uk_df = country_summary_df[country_summary_df['country'].str.lower() != 'united kingdom']

# ğŸ’¾ Export international summary
intl_path = os.path.join(data_export_path, '04_revenue_by_country_excl_uk.csv')
country_excl_uk_df.to_csv(intl_path, index=False)
safe_print(f"âœ… Exported revenue by country (excluding UK): {intl_path}")

# ğŸ“‹ Display top 10 excluding UK
safe_print("\nğŸ“‹ Top 10 Countries by Revenue (Excluding UK):")
display(country_excl_uk_df.head(10))

# ğŸ“Š Plot excluding UK
plt.figure(figsize=(12, 6))
ax = sns.barplot(
    data=country_excl_uk_df.head(10),
    y='country',
    x='total_revenue',
    hue='country',  # âœ… Avoid warning
    legend=False,
    palette='pastel'
)
plt.title("Top 10 Countries by Revenue (Excluding UK)", fontsize=14)
plt.xlabel("Total Revenue (GBP Â£)")
plt.ylabel("Country")

# ğŸ’¬ Add labels
for container in ax.containers:
    ax.bar_label(container, fmt='Â£%.0f', label_type='edge', padding=5)

plt.tight_layout()
plot_path_intl = os.path.join(plot_export_path, '07_country_revenue_bar_excl_uk.png')
plt.savefig(plot_path_intl)
safe_print(f"âœ… Saved plot: {plot_path_intl}")
plt.show()


# ---
# 
# ### â“ Q4: Revenue by Country (With and Without United Kingdom)
# 
# This analysis reveals how sales revenue is distributed geographically, helping identify **top-performing regions** and **international market opportunities**.
# 
# | Insight Category               | Key Observations                                                                 |
# |--------------------------------|----------------------------------------------------------------------------------|
# | ğŸ‡¬ğŸ‡§ **United Kingdom Dominance**       | The UK generated **Â£14.29M** across **33,374 invoices** â€” accounting for **~91%** of all revenue. |
# | ğŸŒ **Top International Market**       | **Ireland (Eire)** leads among non-UK countries, with approximately **Â£586K** in revenue and a high average invoice value of ~**Â£1,111**. |
# | ğŸ’¸ **Highest Avg Invoice Value**      | The **Netherlands** tops in average invoice value at ~**Â£2,545**, despite a moderate volume of 216 invoices. |
# | ğŸ“¦ **Low Volume, High Impact**        | Countries like **Australia**, **Switzerland**, and **Denmark** generate significant revenue from **fewer than 100 invoices** each. |
# | ğŸŒ **Diversified Markets**            | **Germany** and **France** show a healthy balance of volume and value, indicating mature and stable market performance. |
# | ğŸ“ˆ **Regional Opportunity**           | Markets such as **Netherlands**, **Australia**, and **Denmark** are strong candidates for **B2B or premium expansion** due to high-value, low-frequency trends.
# 
# These patterns highlight the importance of maintaining UK market share while strategically expanding into high-potential international regions using **geo-targeted marketing and sales strategies**.
# 
# ---
# 

# ---
# 
# ## ğŸŒ Q5: Do Customer Behaviors Differ by Country?  
# ### Average Spend and Purchase Frequency Analysis
# 
# This analysis helps assess **customer behavior by geography** by comparing two key metrics across countries:
# 
# - **Average spend per customer**  
# - **Average number of invoices per customer**
# 
# ### ğŸ§® Methodology
# We group by `country` and compute:
# - `num_customers`: number of unique customers
# - `num_invoices`: number of unique invoices
# - `total_revenue`: sum of `line_revenue`
# - `avg_invoices_per_customer`: invoices Ã· customers
# - `avg_revenue_per_customer`: revenue Ã· customers
# 
# > ğŸ“‚ Output:
# > - Data export: `eda_outputs/data/05_customer_behavior_by_country.csv`
# > - Plot export: `eda_outputs/plots/08_country_avg_behavior_scatter.png`
# 
# This visualization helps identify:
# - Countries with **high-value, frequent buyers**
# - Regions with **opportunity for engagement or growth**
# 
# ---
# 

# In[12]:


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# ğŸ“ Export paths
data_dir = os.path.join(project_base_path, 'eda_outputs', 'data')
plot_dir = os.path.join(project_base_path, 'eda_outputs', 'plots')
os.makedirs(data_dir, exist_ok=True)
os.makedirs(plot_dir, exist_ok=True)

# ğŸ”§ File paths
data_path = os.path.join(data_dir, '05_customer_behavior_by_country.csv')
plot_path = os.path.join(plot_dir, '08_country_avg_behavior_scatter.png')

# ğŸ“Š Aggregate by country
country_behavior = (
    cleaned_full_df.groupby('country')
    .agg(
        num_customers=('customer_id', 'nunique'),
        num_invoices=('invoice_no', 'nunique'),
        total_revenue=('line_revenue', 'sum')
    )
    .assign(
        avg_invoices_per_customer=lambda df: df['num_invoices'] / df['num_customers'],
        avg_revenue_per_customer=lambda df: df['total_revenue'] / df['num_customers']
    )
    .sort_values(by='total_revenue', ascending=False)
    .reset_index()
)

# ğŸ’¾ Save to CSV
country_behavior.to_csv(data_path, index=False)
safe_print(f"âœ… Exported: {data_path}")

# ğŸ“‹ Display top 10
safe_print("\nğŸ“‹ Top 10 Countries by Avg Spend & Frequency:")
display(country_behavior.head(10))

# ğŸ“ˆ Scatter Plot
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=country_behavior,
    x='avg_invoices_per_customer',
    y='avg_revenue_per_customer',
    size='total_revenue',
    hue='country',
    palette='pastel',
    legend=False,
    sizes=(50, 1000)
)

plt.title('Avg Spend vs Purchase Frequency by Country', fontsize=14, weight='bold')
plt.xlabel('Avg Invoices per Customer')
plt.ylabel('Avg Revenue per Customer (Â£)')
plt.grid(True)
plt.tight_layout()
plt.savefig(plot_path)
safe_print(f"âœ… Saved plot: {plot_path}")
plt.show()


# ### â“ Q5: Avg Spend and Purchase Frequency by Country
# 
# This scatter plot reveals how **average customer behavior** varies by country â€” considering both **purchase frequency** and **total spend**.
# 
# | Insight Category               | Key Observations                                                                 |
# |--------------------------------|----------------------------------------------------------------------------------|
# | ğŸ‡¬ğŸ‡§ **UK Benchmark**                   | The UK has **5,334 customers** with an average of **6.26 invoices** and **Â£2,679 revenue per customer** â€” our largest and most balanced group. |
# | â˜˜ï¸ **Ireland (Eire)**                | With only **3 customers**, Ireland shows **exceptionally high values** (~**Â£195K revenue** and **176 invoices per customer**) â€” likely due to **atypical client behavior** such as a bulk purchasing client, internal use account, or testing/demo user. |
# | ğŸ‡³ğŸ‡± **Netherlands**                  | Strong potential market with **Â£24.9K per customer** and **~10 invoices/customer** â€” indicating high engagement and value. |
# | ğŸ‡«ğŸ‡· ğŸ‡©ğŸ‡ª **France & Germany**           | Both show a **balanced customer base** and solid per-customer revenue (**~Â£3.3Kâ€“Â£3.6K**) â€” suggesting mature buyer relationships. Germany may also support **mid-tier premium strategies**. |
# | ğŸŒ **Premium Segmentation Candidates** | Countries like **Sweden**, **Switzerland**, **Denmark**, and **Australia** show **low-frequency but high-value** purchasing behavior â€” ideal for **premium offerings or B2B targeting**. |
# 
# > âš ï¸ **Note**: Irelandâ€™s high metrics result from **only 3 customers**. These may include a **bulk buyer**, a **test/demo account**, or other **non-typical users**, so interpret with caution.
# 
# These patterns help guide decisions on:
# - Which countries warrant **deeper engagement**
# - Where to test **premium offerings**
# - Where **volume vs. value strategies** might work best
# 

# ---
# 
# ## ğŸ‘¤ Q6: How Many Customers Made Only One Purchase?
# 
# This analysis breaks down customers based on **how many unique invoices** they generated, classifying them as either:
# 
# - **Single Purchase** â€“ exactly one invoice  
# - **Repeat Customer** â€“ more than one invoice
# 
# ### ğŸ§® Methodology
# We group transactions by `customer_id` and:
# - Count the number of unique `invoice_no` per customer
# - Classify each as `"Single Purchase"` or `"Repeat Customer"`
# 
# > ğŸ“‚ Output:
# > - Data export: `eda_outputs/data/06_one_time_vs_repeat_customers.csv`  
# > - Plot export: `eda_outputs/plots/09_one_time_vs_repeat_customers.png`
# 
# ### ğŸ” Why It Matters:
# This breakdown provides a high-level view of **customer retention** and **repeat purchase behavior**. A high proportion of single-purchase customers may indicate:
# - Missed opportunities for re-engagement
# - Seasonal or one-off buyers
# - Gaps in retention strategy or follow-up
# 
# Understanding this split helps prioritize **customer loyalty programs** and refine **post-purchase engagement** efforts.
# 
# ---
# 

# In[13]:


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# ğŸ“ Define paths
data_dir = os.path.join(project_base_path, 'eda_outputs', 'data')
plot_dir = os.path.join(project_base_path, 'eda_outputs', 'plots')
os.makedirs(data_dir, exist_ok=True)
os.makedirs(plot_dir, exist_ok=True)

# ğŸ”§ File outputs
output_csv = os.path.join(data_dir, '06_one_time_vs_repeat_customers.csv')
output_plot = os.path.join(plot_dir, '09_one_time_vs_repeat_customers.png')

# ğŸ“Š Count unique invoices per customer
invoice_counts = (
    cleaned_full_df.groupby('customer_id')
    .agg(
        total_invoices=('invoice_no', 'nunique'),
        total_revenue=('line_revenue', 'sum')
    )
    .reset_index()
)

# ğŸ§© Label customers
invoice_counts['customer_type'] = invoice_counts['total_invoices'].apply(
    lambda x: 'Single Purchase' if x == 1 else 'Repeat Customer'
)

# ğŸ“‹ Summary counts
summary = invoice_counts['customer_type'].value_counts().reset_index()
summary.columns = ['customer_type', 'count']
summary['percent'] = (summary['count'] / summary['count'].sum()) * 100

# ğŸ’¾ Export results
invoice_counts.to_csv(output_csv, index=False)
safe_print(f"âœ… Exported: {output_csv}")

# ğŸ“Š Display summary
display(summary)

# ğŸ“ˆ Pie chart
plt.figure(figsize=(6, 6))
colors = sns.color_palette('pastel')[0:2]
plt.pie(
    summary['count'],
    labels=summary['customer_type'],
    autopct='%1.1f%%',
    colors=colors,
    startangle=140,
    textprops={'fontsize': 12}
)
plt.title('Customer Breakdown: Single vs Repeat Purchasers', fontsize=14, weight='bold')
plt.tight_layout()
plt.savefig(output_plot)
safe_print(f"âœ… Saved plot: {output_plot}")
plt.show()


# ---
# 
# ### â“ Q6: How Many Customers Made Only One Purchase?
# 
# This analysis reveals the proportion of customers who placed **only one order** compared to those who made **repeat purchases**.
# 
# We classified each customer based on the number of unique invoices associated with their ID, and grouped the results as:
# 
# - **Single Purchase** (only one invoice)
# - **Repeat Customer** (two or more invoices)
# 
# | Customer Type      | Count | Percent |
# |--------------------|-------|---------|
# | ğŸŒ€ Repeat Customer  | 4,234 | 72.35%  |
# | ğŸ”¹ Single Purchase  | 1,618 | 27.65%  |
# 
# ---
# 
# ### ğŸ” Key Insights:
# 
# - ğŸŒ€ Nearly **3 out of 4 customers** made **multiple purchases**, suggesting **decent customer retention**.
# - ğŸ”¹ Around **28% of users** only purchased **once**, which presents an opportunity for **re-engagement campaigns**.
# - This segmentation is useful for building **loyalty strategies**, identifying **at-risk users**, and guiding **RFM segmentation** in future analysis.
# 
# 
# ---

# ---
# 
# ## ğŸ’³ Q7: What is the Average Order Value per Customer?
# 
# This question explores **how much each customer spends per order on average**, offering insight into **purchase behavior** and **order economics**.
# 
# ### ğŸ§® Methodology
# 
# - Group by `customer_id`
# - Calculate:
#   - `total_spent`: sum of `line_revenue`
#   - `num_orders`: number of unique `invoice_no`
#   - `avg_order_value`: total_spent Ã· num_orders
# 
# > ğŸ“‚ Output:
# > - Data export: `eda_outputs/data/07_avg_order_value_per_customer.csv`
# > - Plot export: `eda_outputs/plots/10_avg_order_value_distribution.png`
# 
# This metric is useful for:
# 
# - Setting **promotional thresholds** (e.g. free shipping over X)
# - Identifying **high-value customer segments**
# - Comparing order behavior between **B2B and B2C clients**
# 
# ---
# 

# In[14]:


import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ğŸ“ Output folders
data_dir = os.path.join(project_base_path, 'eda_outputs', 'data')
plot_dir = os.path.join(project_base_path, 'eda_outputs', 'plots')
os.makedirs(data_dir, exist_ok=True)
os.makedirs(plot_dir, exist_ok=True)

# ğŸ”§ Define file paths
export_path = os.path.join(data_dir, '07_avg_order_value_per_customer.csv')
plot_path = os.path.join(plot_dir, '10_avg_order_value_distribution.png')  # adjusted index

# ğŸ“Š Compute average order value per customer
avg_order_df = (
    cleaned_full_df.groupby(['customer_id', 'invoice_no'])['line_revenue']
    .sum()
    .reset_index()
    .groupby('customer_id')
    .agg(
        total_spent=('line_revenue', 'sum'),
        num_orders=('invoice_no', 'nunique')
    )
    .assign(avg_order_value=lambda df: df['total_spent'] / df['num_orders'])
    .sort_values(by='avg_order_value', ascending=False)
    .reset_index()
)

# ğŸ’¾ Save summary table
avg_order_df.to_csv(export_path, index=False)
safe_print(f"âœ… Exported: {export_path}")

# ğŸ“‹ Show top 10
safe_print("\nğŸ“‹ Top 10 Customers by Avg Order Value:")
display(avg_order_df.head(10))

# ğŸ“Š Frequency table (raw only)
bins = 15
freq_raw = pd.cut(avg_order_df['avg_order_value'], bins=bins).value_counts().sort_index()
safe_print("\nğŸ“‹ Frequency Table for Avg Order Value:")
display(freq_raw)

# ğŸ¨ Plot: Histogram and Boxplot
fig, axes = plt.subplots(1, 2, figsize=(16, 6))
fig.suptitle("Average Order Value per Customer", fontsize=16, weight="bold")

# Histogram
sns.histplot(avg_order_df['avg_order_value'], bins=100, ax=axes[0], color='mediumaquamarine')
axes[0].set_title("Histogram of Avg Order Value")
axes[0].set_xlabel("Avg Order Value (Â£)")
axes[0].set_ylabel("Number of Customers")

# Boxplot
sns.boxplot(x=avg_order_df['avg_order_value'], ax=axes[1], color='lightgray')
axes[1].set_title("Boxplot of Avg Order Value")
axes[1].set_xlabel("Avg Order Value (Â£)")

# Save and show
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.savefig(plot_path)
safe_print(f"\nâœ… Saved plot: {plot_path}")
plt.show()


# ---
# 
# ### â“ Q7: What is the Average Order Value per Customer?
# 
# We analyzed the **average order value (AOV)** for each customer by dividing their total spending by the number of unique invoices associated with their ID.
# 
# ---
# 
# #### ğŸ§¾ Top 10 Customers by AOV
# 
# The top 10 customers show **exceptionally high average order values** â€” with one customer averaging **Â£84,236.25** per order. These are likely **wholesale buyers**, **event-based purchases**, or other **unusual cases** that stand out from the rest of the customer base.
# 
# | Customer Type     | AOV Example                            |
# |-------------------|----------------------------------------|
# | ğŸ§‘â€ğŸ’¼ Bulk Buyer     | Customer `16446` â†’ **Â£84K** per order     |
# | ğŸ›ï¸ High Spender    | Customer `15749` â†’ **Â£14.8K** per order   |
# | ğŸ“‰ Typical User    | Most customers â†’ **< Â£5,600** per order  |
# 
# ---
# 
# #### ğŸ“‹ Frequency Table
# 
# The frequency table shows that:
# 
# - **99.8%** of customers place orders with an average value **under Â£5,600**
# - Only **10 customers** exceed that threshold
# - One customer falls into the final bin (~**Â£84K**), confirming an **extreme outlier**
# 
# This strong **right-skew** highlights the need to account for outliers when analyzing customer value.
# 
# ---
# 
# #### ğŸ“ˆ Visual Distribution
# 
# The **histogram** and **boxplot** clearly illustrate this skew:
# 
# - The **histogram** shows nearly all customers concentrated in the lower range of AOV
# - The **boxplot** confirms a **tight interquartile range** with several **high-value outliers**
# 
# > These visualizations support robust customer segmentation â€” helping identify **typical users** vs **high-value outliers**, and guiding **loyalty programs**, **discount thresholds**, or **RFM segmentation** strategies.
# 
# ---
# 

# ---
# 
# ### ğŸ’° Q8: Who Are the Top 10 Customers by Total Spend?
# 
# To identify the most valuable customers, we calculate the **total revenue** generated by each customer across all transactions.
# 
# ---
# 
# #### ğŸ§® Methodology
# 
# - Group transactions by `customer_id`
# - Aggregate:
#   - `total_spent`: sum of `line_revenue`
#   - `num_orders`: count of unique `invoice_no`
#   - `avg_order_value`: total_spent Ã· num_orders
# - Sort customers by total_spent in descending order
# - Select the top 10 for detailed review
# 
# > ğŸ“‚ Outputs:
# > - Data export: `08_top_customers_by_total_spend.csv`
# > - Plot export: `11_top_customers_by_total_spend.png`
# 
# ---
# 
# #### ğŸ“Š Key Business Value
# 
# - Identifies **high-value clients** for **retention**, **loyalty programs**, and **account-based marketing**
# - Helps assess how much revenue depends on a few top buyers
# - Supports **priority segmentation** and **custom communication strategies**
# 
# ---
# 
# #### ğŸ“‹ Metrics Reported per Customer
# 
# | Metric             | Description                                       |
# |--------------------|---------------------------------------------------|
# | `customer_id`      | Unique identifier                                 |
# | `total_spent`      | Total revenue across all purchases                |
# | `num_orders`       | Number of unique invoices                         |
# | `avg_order_value`  | Average revenue per order (total_spent Ã· orders)  |
# 
# The top 10 customers offer valuable insight into **spending patterns**, **engagement level**, and **strategic account value**. They should be prioritized for retention and further analysis.
# 
# ---
# 

# In[15]:


import os
import pandas as pd
import matplotlib.pyplot as plt

# ğŸ“ Define output paths
data_dir = os.path.join(project_base_path, 'eda_outputs', 'data')
plot_dir = os.path.join(project_base_path, 'eda_outputs', 'plots')
os.makedirs(data_dir, exist_ok=True)
os.makedirs(plot_dir, exist_ok=True)

data_output_path = os.path.join(data_dir, '08_top_customers_by_total_spend.csv')
plot_output_path = os.path.join(plot_dir, '11_top_customers_by_total_spend.png')

# ğŸ§® Top spenders calculation
top_spenders_df = (
    cleaned_full_df.groupby('customer_id')
    .agg(
        total_spent=('line_revenue', 'sum'),
        num_orders=('invoice_no', 'nunique')
    )
    .assign(avg_order_value=lambda df: df['total_spent'] / df['num_orders'])
    .sort_values(by='total_spent', ascending=False)
    .reset_index()
    .head(10)
)

# ğŸ’¾ Save data
top_spenders_df.to_csv(data_output_path, index=False)
safe_print(f"âœ… Exported: {data_output_path}")

# ğŸ“‹ Display
safe_print("\nğŸ“‹ Top 10 Customers by Total Spend:")
display(top_spenders_df)

# ğŸ¨ Plot using Matplotlib directly (avoid seabornâ€™s layout bloat)
fig, ax = plt.subplots(figsize=(10, 6))
sorted_df = top_spenders_df.sort_values(by='total_spent')  # Plot from lowest to highest for horizontal bars

ax.barh(sorted_df['customer_id'].astype(str), sorted_df['total_spent'], color='lightblue')
ax.set_xlabel('Total Spend (GBP Â£)')
ax.set_ylabel('Customer ID')
ax.set_title('Top 10 Customers by Total Spend', fontsize=14, weight='bold')

# ğŸ’¬ Annotate bars
for i, value in enumerate(sorted_df['total_spent']):
    ax.text(value + 1000, i, f"Â£{value:,.0f}", va='center', fontsize=9)

plt.tight_layout()
plt.savefig(plot_output_path)
safe_print(f"âœ… Saved plot: {plot_output_path}")
plt.show()


# 
# ---
# 
# ### â“ Q8: Who Are the Top 10 Customers by Total Spend?
# 
# We identified the **most valuable customers** by calculating the **total revenue** each has generated across all invoices.
# 
# ---
# 
# #### ğŸ§¾ Top 10 Customers by Lifetime Value
# 
# The table below lists the top-spending customers between 2009 and 2011. These clients account for a **significant portion of total revenue**.
# 
# | Metric             | Description |
# |--------------------|-------------|
# | `total_spent`      | Total revenue across all purchases |
# | `num_orders`       | Unique invoice count per customer |
# | `avg_order_value`  | Average spend per order (total_spent Ã· orders) |
# 
# Key observations:
# 
# - ğŸ¥‡ **Customer 18102** tops the list with **Â£580,987** across 145 orders.
# - ğŸ’¼ Several others (e.g., `14646`, `14156`) show high revenue from **consistent repeat ordering**.
# - ğŸ“¦ **Customer `16446`** stands out with just **2 orders** but **Â£168K total spend** â€” likely a **bulk B2B transaction or anomaly**.
# - ğŸ§  These customers warrant **special attention** for retention, loyalty programs, or custom offers.
# 
# ---
# 
# #### ğŸ“ˆ Visual Distribution
# 
# The horizontal bar chart below shows the top 10 spenders in descending order:
# 
# - Highlights **relative revenue contributions**
# - Annotated with actual GBP totals for clarity
# - Sorted for visual impact and comparison
# 
# > These insights are essential for **customer segmentation**, prioritizing **high-value accounts**, and designing **tiered marketing strategies** based on revenue contribution.
# 
# ---
# 

# ---
# 
# ### ğŸ•’ Q9: How Recently Has Each Customer Made a Purchase?
# 
# This step measures the **Recency** of each customer by calculating the number of days since their **last purchase**, relative to the most recent transaction in the dataset.
# 
# ---
# 
# #### ğŸ§® Methodology:
# 
# - Group transactions by `customer_id`
# - Find the **most recent `invoice_date`** for each customer
# - Define the **reference date** as the **latest invoice date** in the dataset
# - Compute **Recency** as the number of days between the reference date and each customer's last purchase
# 
# > ğŸ“‚ Outputs:
# > - Data export: `09_customer_recency.csv`
# > - Plot export: `12_customer_recency_distribution.png`
# 
# ---
# 
# #### ğŸ“Š Business Value:
# 
# - Helps identify **active vs. inactive users**
# - Supports **re-engagement campaigns** targeting dormant customers
# - Lays the foundation for **RFM segmentation**
# 
# ---
# 
# #### ğŸ“ˆ What We Expect to See:
# 
# - Many customers will have made purchases close to the reference date  
# - Some will have long gaps since their last purchase (potential churn risk)  
# - Weâ€™ll visualize this distribution using a histogram and boxplot
# 
# ---

# In[16]:


import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ğŸ“ Define export directories
data_dir = os.path.join(project_base_path, 'eda_outputs', 'data')
plot_dir = os.path.join(project_base_path, 'eda_outputs', 'plots')
os.makedirs(data_dir, exist_ok=True)
os.makedirs(plot_dir, exist_ok=True)

# ğŸ”§ Define file paths
recency_csv = os.path.join(data_dir, '09_customer_recency.csv')
recency_plot = os.path.join(plot_dir, '12_customer_recency_distribution.png')

# ğŸ•’ Reference date = latest invoice date in dataset
reference_date = cleaned_full_df['invoice_date'].max()

# ğŸ§® Calculate recency per customer
recency_df = (
    cleaned_full_df.groupby('customer_id')['invoice_date']
    .max()
    .reset_index()
    .assign(recency_days=lambda df: (reference_date - df['invoice_date']).dt.days)
    .sort_values(by='recency_days')
)

# ğŸ’¾ Save to CSV
recency_df.to_csv(recency_csv, index=False)
safe_print(f"âœ… Exported: {recency_csv}")

# ğŸ“‹ Show first and last 5 rows for context
safe_print("\nğŸ“‹ Sample Customers by Recency (Top & Bottom):")
display(recency_df.head(5))
display(recency_df.tail(5))

# ğŸ¨ Plot distribution
plt.figure(figsize=(14, 5))

# Histogram
plt.subplot(1, 2, 1)
sns.histplot(recency_df['recency_days'], bins=40, kde=False, color='skyblue')
plt.title("Distribution of Customer Recency")
plt.xlabel("Days Since Last Purchase")
plt.ylabel("Number of Customers")

# Boxplot
plt.subplot(1, 2, 2)
sns.boxplot(x=recency_df['recency_days'], color='lightgray')
plt.title("Boxplot of Customer Recency")
plt.xlabel("Days Since Last Purchase")

plt.tight_layout()
plt.savefig(recency_plot)
safe_print(f"âœ… Saved plot: {recency_plot}")
plt.show()


# ---
# 
# ### â“ Q9: How Recently Has Each Customer Made a Purchase?
# 
# We calculated each customer's **Recency** â€” the number of days since their most recent purchase â€” relative to the datasetâ€™s latest invoice date (**2011-12-09**).
# 
# ---
# 
# #### ğŸ§® Key Observations
# 
# - ğŸŸ¢ Multiple customers made purchases on the **final day of available data** (`recency_days = 0`)
# - ğŸ“‰ Most customers made their most recent purchase within the **first 100 days**
# - ğŸŸ¡ Recency spans from **0 to 738 days**, with a long right tail
# - ğŸ“¦ A small number of customers havenâ€™t purchased in **over 2 years**, signaling potential **churn** or **inactive accounts**
# 
# ---
# 
# #### ğŸ“‹ Sample Customers by Recency (Top & Bottom)
# 
# The table below shows examples of both **active** and **inactive** customers:
# 
# - Customers with `recency_days = 0` â†’ recently engaged
# - Customers with `recency_days > 700` â†’ dormant or churned
# 
# These recency values will help define **RFM segmentation** thresholds and identify targets for **retention campaigns**.
# 
# ---
# 
# #### ğŸ“Š Visual Insights
# 
# - The **histogram** confirms a strong cluster of customers with low recency (recent buyers)
# - The **boxplot** highlights a **highly skewed distribution**, with many long-inactive customers far from the median
# 
# This analysis lays the foundation for more effective segmentation and **customer lifecycle marketing**.
# 
# ---
# 

# ---
# 
# ### ğŸ” Q10: How Frequently Has Each Customer Purchased?
# 
# This step calculates the **Frequency** of each customer by counting the number of **unique invoices** they generated throughout the dataset.
# 
# ---
# 
# #### ğŸ§® Methodology:
# 
# - Group the data by `customer_id`
# - Count the number of **unique `invoice_no`** per customer
# - Define Frequency as the total number of **distinct purchase events**
# 
# > ğŸ“‚ Outputs:
# > - Data export: `10_customer_frequency.csv`
# > - Plot export: `13_customer_frequency_distribution.png`
# 
# ---
# 
# #### ğŸ“Š Business Value:
# 
# - Identifies **repeat vs. one-time customers**
# - Enables segmentation of **high-engagement** users
# - Supports **customer loyalty** analysis and **lifecycle marketing**
# 
# ---
# 
# #### ğŸ“ˆ What We Expect to See:
# 
# - Many customers may have purchased only once
# - A smaller segment may be **frequent or loyal shoppers**
# - The histogram and boxplot will reveal the shape and skewness of purchase frequency
# 
# ---
# 

# In[17]:


import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# âœ… Define export paths
export_path = os.path.join(data_dir, '10_customer_frequency.csv')
plot_path = os.path.join(plot_dir, '13_customer_frequency_distribution.png')

# ğŸ“Š Calculate customer frequency
frequency_df = (
    cleaned_full_df.groupby('customer_id')
    .agg(
        num_orders=('invoice_no', 'nunique'),
        total_spent=('line_revenue', 'sum')
    )
    .assign(avg_order_value=lambda df: df['total_spent'] / df['num_orders'])
    .sort_values(by='num_orders', ascending=False)
    .reset_index()
)

# ğŸ’¾ Export CSV
frequency_df.to_csv(export_path, index=False)
safe_print(f"âœ… Exported: {export_path}")

# ğŸ“‹ Show sample customers (top & bottom)
safe_print("\nğŸ“‹ Sample Customers by Frequency (Top & Bottom):")
display(frequency_df.head())
display(frequency_df.tail())

# ğŸ“Š Create bin edges (50 bins) and compute histogram
bin_count = 50
counts, bin_edges = np.histogram(frequency_df['num_orders'], bins=bin_count)

# ğŸ“‹ Display frequency table using histogram counts
freq_table = pd.Series(counts, index=pd.IntervalIndex.from_breaks(bin_edges))
safe_print("\nğŸ“‹ Frequency Table for Number of Orders:")
display(freq_table)

# ğŸ¨ Plot histogram and boxplot
fig, axes = plt.subplots(1, 2, figsize=(16, 5))
fig.suptitle("Customer Purchase Frequency", fontsize=18, weight="bold")

# Histogram (manual bar plot)
axes[0].bar(x=bin_edges[:-1], height=counts, width=np.diff(bin_edges), color='skyblue', align='edge', edgecolor='gray')
axes[0].set_title("Distribution of Purchase Frequency")
axes[0].set_xlabel("Number of Orders")
axes[0].set_ylabel("Number of Customers")

# Boxplot
sns.boxplot(x=frequency_df['num_orders'], ax=axes[1], color='lightgray')
axes[1].set_title("Boxplot of Purchase Frequency")
axes[1].set_xlabel("Number of Orders")

# Save and show
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.savefig(plot_path)
safe_print(f"âœ… Saved plot: {plot_path}")
plt.show()


# ---
# 
# ### â“ Q10: How Frequently Has Each Customer Purchased?
# 
# We measured **Frequency** as the total number of **unique orders (`invoice_no`)** made by each customer during the observed period.
# 
# ---
# 
# #### ğŸ§® Key Observations:
# 
# - ğŸŸ¦ **Most customers** placed **between 1 and 8 orders**, with **4,735 users** falling into this initial bin.
# - ğŸŸ¡ A few customers placed more than **100** orders â€” with the most active buyer reaching **373 invoices**.
# - ğŸ“‰ The **distribution is right-skewed**, suggesting the presence of a small group of **power users** amid a broader base of casual shoppers.
# 
# ---
# 
# #### ğŸ“Š Visual Insights:
# 
# - The **histogram** shows a steep drop after the first bin, confirming that **low-frequency shoppers dominate** the user base.
# - The **boxplot** visually highlights **numerous outliers**, helping spot highly engaged or anomalous customers.
# 
# > These findings are valuable for **customer segmentation**, **loyalty analysis**, and defining tailored **marketing strategies** for frequent buyers.
# 
# ---
# 

# ---
# 
# ### ğŸ’° Q11: What is the Monetary Value of Each Customer?
# 
# We calculated **Monetary Value** as the total amount each customer spent across all orders in the dataset.
# 
# ---
# 
# #### ğŸ§® Methodology:
# 
# - Group transactions by `customer_id`
# - Sum the `line_revenue` per customer to compute total spend
# - Sort to identify **top-spending users**
# 
# > ğŸ“‚ Outputs:
# > - Data export: `11_customer_monetary_value.csv`
# > - Plot export: `14_customer_monetary_value_distribution.png`
# 
# ---
# 
# #### ğŸ“Š Business Value:
# 
# - Helps identify **high-value customers** for VIP programs or exclusive offers
# - Enables **RFM segmentation** by combining Recency, Frequency, and Monetary metrics
# - Useful for calculating **Customer Lifetime Value (CLV)** when paired with time windows
# 
# ---
# 
# #### ğŸ“ˆ What We Expect to See:
# 
# - A right-skewed distribution, where most customers spend a relatively low amount
# - A few standout customers contributing a significant portion of total revenue
# 
# ---
# 

# In[18]:


# ğŸ“¦ Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

# âœ… Setup paths
data_dir = os.path.join(project_base_path, 'eda_outputs', 'data')
plot_dir = os.path.join(project_base_path, 'eda_outputs', 'plots')
os.makedirs(data_dir, exist_ok=True)
os.makedirs(plot_dir, exist_ok=True)

# ğŸ”§ File paths
export_path = os.path.join(data_dir, '11_customer_monetary_value.csv')
plot_path = os.path.join(plot_dir, '14_customer_monetary_value_distribution.png')

# ğŸ“¥ Load frequency data
frequency_path = os.path.join(data_dir, '10_customer_frequency.csv')
frequency_df = pd.read_csv(frequency_path)

# âœ… Sort by total_spent descending
frequency_df = frequency_df.sort_values(by='total_spent', ascending=False).reset_index(drop=True)

# ğŸ’¾ Save again under monetary value filename (for consistency)
frequency_df.to_csv(export_path, index=False)
safe_print(f"âœ… Exported: {export_path}")

# ğŸ“‹ Show sample rows
safe_print("\nğŸ“‹ Sample Customers by Monetary Value (Top & Bottom):")
display(frequency_df[['customer_id', 'total_spent', 'num_orders', 'avg_order_value']].head())
display(frequency_df[['customer_id', 'total_spent', 'num_orders', 'avg_order_value']].tail())

# ğŸ“Š Frequency table with ~Â£20K bins
bin_count = 30
bin_edges = np.histogram_bin_edges(frequency_df['total_spent'], bins=bin_count)
freq_table = pd.cut(frequency_df['total_spent'], bins=bin_edges).value_counts().sort_index()
safe_print("\nğŸ“‹ Frequency Table for Total Spend:")
display(freq_table.to_frame())

# ğŸ¨ Plot histogram and boxplot
fig, axes = plt.subplots(1, 2, figsize=(16, 5))
fig.suptitle("Customer Monetary Value", fontsize=18, weight="bold")

# Histogram
sns.histplot(frequency_df['total_spent'], bins=bin_edges, ax=axes[0], color='mediumseagreen')
axes[0].set_title("Distribution of Total Spend")
axes[0].set_xlabel("Total Spend (Â£)")
axes[0].set_ylabel("Number of Customers")

# Boxplot
sns.boxplot(x=frequency_df['total_spent'], ax=axes[1], color='lightgray')
axes[1].set_title("Boxplot of Total Spend")
axes[1].set_xlabel("Total Spend (Â£)")

# Save and show plot
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.savefig(plot_path)
safe_print(f"âœ… Saved plot: {plot_path}")
plt.show()


# ---
# 
# ### â“ Q11: How Much Has Each Customer Spent?
# 
# This analysis measures **Monetary Value**, representing the total amount each customer spent over the observed period.
# 
# ---
# 
# #### ğŸ“‹ Key Observations:
# 
# - ğŸŸ¢ **Most customers are low spenders**: Over **5,700 customers (99%)** spent less than **Â£20,000**, with a strong peak concentrated at the **low end of the total spend range**.
# - ğŸŸ¡ **High spenders are rare but significant**: Around a **dozen customers** spent more than **Â£100,000**, and the top customer spent **Â£580K** â€” highlighting the presence of a **very high-value segment** with potential for personalized retention strategies.
# - ğŸ” The distribution is **extremely right-skewed**, suggesting that a few high-spend customers generate a disproportionately large share of revenue.
# - ğŸ§® The median total spend is likely low, but the **long tail of outliers** extends beyond **Â£500K**.
# 
# ---
# 
# #### ğŸ“Š Visual Insights:
# 
# - The **histogram** shows a steep drop-off after the first bin, reinforcing the dominance of lower-spending customers.
# - The **boxplot** highlights significant outliers â€” a small group with unusually high monetary value that could be explored for loyalty or retention initiatives.
# 
# > These insights are critical for **customer segmentation**, identifying **top-value customers**, and designing **tiered loyalty programs**.
# 
# ---

# ---
# 
# ### ğŸ§© Q12: How Can We Segment Customers Based on RFM Scores?
# 
# This final step uses **RFM segmentation** to categorize customers based on their **Recency**, **Frequency**, and **Monetary** behavior.
# 
# ---
# 
# #### ğŸ§® Methodology:
# 
# - **Recency**: Days since the last purchase  
# - **Frequency**: Total number of unique invoices  
# - **Monetary**: Total amount spent  
# 
# Each metric is assigned a score from **1 (low)** to **4 (high)** based on **quartiles**. We then define the following key segments:
# 
# | Segment         | Criteria                        |
# |-----------------|----------------------------------|
# | ğŸŸ¢ Loyal         | High Recency + High Frequency   |
# | ğŸŸ¡ High-Value    | High Frequency + High Monetary  |
# | ğŸ”´ At-Risk       | Low Recency                     |
# | ğŸ”µ One-Time Buyer| Frequency = 1                   |
# | âšª Other         | Doesn't meet any specific group |
# 
# > ğŸ“‚ Outputs:
# > - Data export: `12_rfm_segmented_customers.csv`  
# > - Plot export: `15_rfm_segment_distribution.png`
# 
# ---
# 
# #### ğŸ“Š Business Value:
# 
# - Enables **targeted marketing campaigns**
# - Identifies opportunities for **customer retention and reactivation**
# - Supports **loyalty programs** and **priority service tiers**
# 
# ---
# 
# #### ğŸ” What We Expect to See:
# 
# - A high proportion of customers with low Frequency and Recency  
# - A small group of **loyal** and **high-value** customers  
# - Clear distinctions in behavior that justify different engagement strategies
# 
# ---
# 

# In[19]:


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os

# âœ… Paths
recency_csv = os.path.join(data_dir, '09_customer_recency.csv')  # from Q9
export_path = os.path.join(data_dir, '12_rfm_segmented_customers.csv')
plot_path = os.path.join(plot_dir, '15_rfm_segment_distribution.png')

# ğŸ“¥ Load precomputed recency table
recency_df = pd.read_csv(recency_csv)

# ğŸ§® Compute frequency and monetary from original data
rfm_base = (
    cleaned_full_df.groupby('customer_id')
    .agg(
        frequency=('invoice_no', 'nunique'),
        monetary=('line_revenue', 'sum')
    )
    .reset_index()
)

# ğŸ”— Merge with recency
rfm_df = pd.merge(rfm_base, recency_df[['customer_id', 'recency_days']], on='customer_id')
rfm_df.rename(columns={'recency_days': 'recency'}, inplace=True)

# ğŸ·ï¸ RFM Scoring (quartiles)
rfm_df['R'] = pd.qcut(rfm_df['recency'], 4, labels=[4, 3, 2, 1]).astype(int)
rfm_df['F'] = pd.qcut(rfm_df['frequency'].rank(method='first'), 4, labels=[1, 2, 3, 4]).astype(int)
rfm_df['M'] = pd.qcut(rfm_df['monetary'], 4, labels=[1, 2, 3, 4]).astype(int)

# ğŸ§® RFM Score
rfm_df['RFM_Score'] = rfm_df[['R', 'F', 'M']].sum(axis=1)

# ğŸ§  Assign RFM Segment
def assign_segment(row):
    if row['RFM_Score'] >= 9:
        return 'High-Value'
    elif row['R'] >= 3 and row['F'] >= 3:
        return 'Loyal'
    elif row['R'] == 1:
        return 'At-Risk'
    elif row['F'] == 1 and row['M'] == 1:
        return 'One-Time'
    else:
        return 'Other'

rfm_df['Segment'] = rfm_df.apply(assign_segment, axis=1)

# ğŸ’¾ Export CSV
rfm_df.to_csv(export_path, index=False)
safe_print(f"âœ… Exported: {export_path}")

# âœ… Show a sample of the exported RFM scores
rfm_preview = pd.read_csv(export_path)
safe_print("\nğŸ“‹ Sample of RFM Score Table:")
display(rfm_preview.head(10))  # or .sample(10) for randomness

# ğŸ“‹ Show segment frequency table before chart
safe_print("\nğŸ“‹ RFM Segment Counts:")
segment_counts = rfm_df['Segment'].value_counts().sort_values(ascending=False)
display(segment_counts)

# ğŸ¨ Plot: Segment Distribution (fixes future warning)
plt.figure(figsize=(10, 6))
sns.barplot(
    x=segment_counts.index,
    y=segment_counts.values,
    hue=segment_counts.index,  # required for palette to apply
    palette='Set2',
    legend=False  # disables auto-legend
)
plt.title("Customer Segments Based on RFM Scores", fontsize=16, weight="bold")
plt.xlabel("RFM Segment")
plt.ylabel("Number of Customers")
plt.xticks(rotation=30)
plt.tight_layout()
plt.savefig(plot_path)
safe_print(f"âœ… Saved plot: {plot_path}")
plt.show()


# ---
# 
# ### â“ Q12: How Can We Segment Customers Based on RFM Scores?
# 
# We performed an **RFM segmentation** using SQL logic and pandas scoring to categorize customers based on their **Recency**, **Frequency**, and **Monetary** behavior.
# 
# ---
# 
# #### ğŸ“‹ Segment Summary:
# 
# | Segment         | Description                                       |
# |-----------------|---------------------------------------------------|
# | ğŸ”´ **At-Risk**     | Long time since last purchase (low Recency)      |
# | ğŸŸ¢ **Loyal**       | Frequent and recent buyers (high R + F)          |
# | ğŸŸ¡ **High-Value**  | High spending and frequent (high M + F)          |
# | ğŸ”µ **One-Time**    | Single-purchase customers (F = 1 and M = 1)       |
# | âšª **Other**        | Doesnâ€™t meet specific criteria for grouping      |
# 
# ---
# 
# #### ğŸ“Š Visual Insights:
# 
# - The largest group is **High-Value**, followed by **Other** and **At-Risk**.
# - **Loyal** and **One-Time** customers form smaller but strategically important segments.
# - This distribution reveals a **diverse customer base** with distinct behavioral patterns.
# 
# > These insights can support **personalized retention strategies**, **targeted promotions**, and the **prioritization of high-value or loyal customers**.
# 
# ---
# 

# ### ğŸ“‹ Summary of Exploratory Data Analysis
# 
# In this notebook, we conducted a comprehensive Exploratory Data Analysis (EDA) of the cleaned Online Retail II dataset. Our analysis focused on identifying key trends in sales performance, customer behavior, and country-level insights.
# 
# Key findings include:
# 
# - **Sales Performance**:
#   - The dataset contains over **776,000** transactions across **38 countries**.
#   - Sales peaked in **November 2011**, with notable seasonal spikes during holiday months.
#   - The **top-selling products** include **decorative and seasonal items** such as *Regency Cakestand*, *Chilli Lights*, and *T-Light Holders*, along with **popular packaging and craft items** like *Jumbo Bags* and *Paper Craft Kits*.
# 
# - **Country Insights**:
#   - The **UK dominates** both in total revenue and invoice volume.
#   - Excluding the UK, **Ireland, the Netherlands, and Germany** are the strongest markets by revenue.
#   - Some small countries (e.g., Australia, Switzerland) showed high **average invoice values**, suggesting fewer but larger orders.
# 
# - **Customer Insights**:
#   - About **72%** of customers are **repeat buyers**, while **28%** made only one purchase.
#   - Top customers spent up to **Â£580,000**, with some placing over **300 orders**.
#   - Distribution of **recency, frequency, and monetary value** highlights a small group of highly engaged, high-spending customers.
# 
# - **Product and Invoice Patterns**:
#   - Most transactions involve 1â€“12 units per item.
#   - A small number of invoices and products account for a large share of total revenue, consistent with the **Pareto principle** (80/20 rule).
# 
# ---
# 
# ### ğŸ’¼ Business Recommendations
# 
# Based on our findings, we suggest the following actions to improve sales and customer retention:
# 
# 1. **Focus on High-Value Customers**  
#    Target the top spenders with personalized marketing, loyalty perks, or exclusive early access to new collections.
# 
# 2. **Nurture Repeat Buyers**  
#    Since a majority of customers are repeat purchasers, incentivize additional purchases through tailored promotions or bundling offers.
# 
# 3. **Capitalize on Holiday Spikes**  
#    Strengthen inventory and advertising campaigns in Q4, especially in October and November, to capture peak seasonal demand.
# 
# 4. **Expand Beyond the UK**  
#    Explore marketing and logistics strategies to grow in **Ireland**, **Netherlands**, and **Germany**, where order value and revenue are promising.
# 
# 5. **Optimize Product Mix**  
#    Leverage insights from top-performing products to guide future inventory planning and new product development.
# 
# 6. **Monitor Inactive Customers**  
#    Implement win-back campaigns for customers who havenâ€™t purchased in several months, based on recency data.
# 

# ---
# 
# ## ğŸ§© Optional Script Entry Point
# 
# This block allows the EDA notebook to run as a standalone `.py` script.
# 
# When executed directly (e.g. from terminal or a job scheduler), all analysis steps, visualizations, and exports will be completed automatically â€” supporting reproducible analytics workflows.
# 
# ---
# 

# In[20]:


# âœ… Optional script execution indicator for CLI use
if __name__ == "__main__":
    safe_print("ğŸ“Š EDA script executed directly as a .py file â€” all analysis steps and exports have been completed.")



# ------------------------------------------------------------------------------
# ğŸ›¡ï¸ License & Attribution
#
# Â© 2024 Ginosca Alejandro DÃ¡vila
# Project: Online Retail II â€“ Sales Analysis & Customer Segmentation
# ğŸ« School: Ironhack Puerto Rico
# ğŸ“ Bootcamp: Data Science and Machine Learning
# ğŸ“… Date: December 20, 2024
# This work is provided for educational purposes under the MIT License.
# You may reuse, modify, or redistribute with attribution.
# ------------------------------------------------------------------------------
